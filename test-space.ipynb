{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e69d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embs = np.load('/home/hongn/Desktop/sapiens/pretrain/Outputs/vis/itw_videos/reel1_pretrain/sapiens_0.3b/000018.npy')\n",
    "\n",
    "flows = np.load('/home/hongn/RAFT_clone/flows_sample.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb676b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flows['arr_0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "batch, length, dim = 2, 64, 16\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "model = Mamba(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=dim, # Model dimension d_model\n",
    "    d_state=16,  # SSM state expansion factor\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    ").to(\"cuda\")\n",
    "y = model(x)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223f3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongn/miniconda3/envs/videollm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from mamba_ssm import Mamba  # Assumed available\n",
    "import vision_transformer as vits\n",
    "from einops import rearrange\n",
    "\n",
    "class MaMoose(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024, hidden_dim=512, patch_hw=64, sequence_len=100):\n",
    "        super(MaMoose, self).__init__()\n",
    "        self.sequence_len = sequence_len\n",
    "        self.patch_hw = patch_hw\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Flatten spatial patches and apply linear projection\n",
    "        self.input_proj = nn.Linear(embedding_dim, hidden_dim)\n",
    "\n",
    "        # Mamba operates on [B, L, D]\n",
    "        self.mamba = Mamba(d_model=hidden_dim)\n",
    "\n",
    "        # Output classifier: simple binary classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.motion_feature_extractor  = vits.__dict__['vit_tiny'](patch_size=2, num_classes=0, in_chans=2, img_size=(128, 128))\n",
    "        \n",
    "    def flow_encoder(self, optical_list):\n",
    "        b, t = optical_list.shape[0], optical_list.shape[1]\n",
    "        x = rearrange(optical_list, 'b t c w h -> (b t) c w h')\n",
    "        of_embs = self.motion_feature_extractor(x)\n",
    "        return of_embs\n",
    "\n",
    "    def forward(self, optical_list, visual_list=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optical_list: list of tensors [T, H, W, C], each with shape [T=100, 2, W=224, H=224]\n",
    "        Returns:\n",
    "            logits: [B, 1]\n",
    "        \"\"\"\n",
    "        # B = len(optical_list)\n",
    "        device = optical_list[0].device\n",
    "\n",
    "        # Stack and sum inputs: shape [B, T, H, W, C]\n",
    "        optical = optical_list  # [B, 100, 64, 64, 1024]\n",
    "        # visual = torch.stack(visual_list)\n",
    "        \n",
    "        # b, t = optical.shape[0], optical.shape[1]\n",
    "        # optical = rearrange(optical, 'b t c w h -> (b t) c w h')\n",
    "        flow_embs = self.flow_encoder(optical)\n",
    "        # flow_embs = rearrange(flow_embs, '(b t) e -> b t e', b=b, t=t)\n",
    "\n",
    "        # Binary classification\n",
    "        # logits = self.classifier(x)  # [B, 1]\n",
    "\n",
    "        return flow_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7caf3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 192])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vision_transformer as vits\n",
    "import torch\n",
    "\n",
    "model = MaMoose().cuda()\n",
    "x = torch.randn(2, 10, 2, 128, 128).cuda()\n",
    "y = model(x)\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6545b524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 400])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from mamba_ssm import Mamba  # Assumed available\n",
    "import vision_transformer as vits\n",
    "from einops import rearrange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class SmallFlowEncoder(nn.Module):\n",
    "    def __init__(self, output_dim=192):\n",
    "        super().__init__()\n",
    "        # Use pretrained or reduced ResNet (remove final FC)\n",
    "        base_resnet = models.resnet18(pretrained=False)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_resnet.children())[:-1])  # [B, 512, 1, 1]\n",
    "        self.fc = nn.Linear(512, output_dim)\n",
    "\n",
    "    def forward(self, flow_seq):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            flow_seq: [T, 2, W, H] or [B, T, 2, W, H]\n",
    "        Returns:\n",
    "            embeddings: [T, output_dim] or [B, T, output_dim]\n",
    "        \"\"\"\n",
    "        if flow_seq.dim() == 4:  # [T, 2, W, H]\n",
    "            T = flow_seq.shape[0]\n",
    "            x = self._prepare_input(flow_seq)  # [T, 3, W, H]\n",
    "            features = self.feature_extractor(x).squeeze(-1).squeeze(-1)  # [T, 512]\n",
    "            return self.fc(features)  # [T, output_dim]\n",
    "\n",
    "        elif flow_seq.dim() == 5:  # [B, T, 2, W, H]\n",
    "            B, T, _, W, H = flow_seq.shape\n",
    "            flow_seq = flow_seq.reshape(B * T, 2, W, H)\n",
    "            x = self._prepare_input(flow_seq)  # [B*T, 3, W, H]\n",
    "            features = self.feature_extractor(x).squeeze(-1).squeeze(-1)  # [B*T, 512]\n",
    "            features = self.fc(features).reshape(B, T, -1)  # [B, T, output_dim]\n",
    "            return features\n",
    "\n",
    "    def _prepare_input(self, flow):\n",
    "        # Pad to 3 channels: [N, 2, W, H] â†’ [N, 3, W, H]\n",
    "        N, C, W, H = flow.shape\n",
    "        if C == 2:\n",
    "            pad = torch.zeros((N, 1, W, H), device=flow.device)\n",
    "            flow = torch.cat([flow, pad], dim=1)\n",
    "        return flow\n",
    "\n",
    "\n",
    "class MaMoose(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024, hidden_dim=192, patch_hw=64, sequence_len=10, num_classes=400):\n",
    "        super(MaMoose, self).__init__()\n",
    "        self.sequence_len = sequence_len\n",
    "        self.patch_hw = patch_hw\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Flatten spatial patches and apply linear projection\n",
    "        self.input_proj = nn.Linear(embedding_dim, hidden_dim)\n",
    "\n",
    "        # Mamba operates on [B, L, D]\n",
    "        self.mamba  = Mamba(\n",
    "            # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "            d_model=hidden_dim, # Model dimension d_model\n",
    "            d_state=16,  # SSM state expansion factor\n",
    "            d_conv=4,    # Local convolution width\n",
    "            expand=2,    # Block expansion factor\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # Output classifier: simple binary classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "        self.motion_feature_extractor  = SmallFlowEncoder() #vits.__dict__['vit_tiny'](patch_size=2, num_classes=0, in_chans=2, img_size=(128, 128))\n",
    "        \n",
    "    def flow_encoder(self, optical_list):\n",
    "        b, t = optical_list.shape[0], optical_list.shape[1]\n",
    "        x = rearrange(optical_list, 'b t c w h -> (b t) c w h')\n",
    "        X = self.motion_feature_extractor(x)\n",
    "        x = rearrange(X, '(b t) e -> b t e', b=b, t=t)\n",
    "        return x\n",
    "\n",
    "    def forward(self, optical_list, visual_list=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optical_list: list of tensors [T, H, W, C], each with shape [100, 2, 224, 224]\n",
    "        Returns:\n",
    "            logits: [B, 1]\n",
    "        \"\"\"\n",
    "        # B = len(optical_list)\n",
    "        device = optical_list[0].device\n",
    "\n",
    "        # Stack and sum inputs: shape [B, T, H, W, C]\n",
    "        optical = optical_list  # [B, 100, 64, 64, 1024]\n",
    "        # visual = torch.stack(visual_list)\n",
    "        \n",
    "        # b, t = optical.shape[0], optical.shape[1]\n",
    "        # optical = rearrange(optical, 'b t c w h -> (b t) c w h')\n",
    "        flow_embs = self.flow_encoder(optical) #[100, 192] \n",
    "        # x = concat[flow, visual]\n",
    "        x = self.mamba(flow_embs)\n",
    "        # flow_embs = rearrange(flow_embs, '(b t) e -> b t e', b=b, t=t)\n",
    "\n",
    "        # # Step 1: Project to class logits\n",
    "        logits = self.classifier(x)  # [B, T, C]\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = MaMoose().cuda()\n",
    "x = torch.randn(2, 100, 2, 256, 256).cuda()\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_encoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videollm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
